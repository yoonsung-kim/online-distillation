{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import models, datasets, transforms\n",
    "\n",
    "from test.models import *\n",
    "from utils.losses import OnlineDistillationLoss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "def train(model,\n",
    "          loss_function,\n",
    "          train_data_loader,\n",
    "          valid_data_loader,\n",
    "          target_valid_accuracy,\n",
    "          epochs,\n",
    "          learning_rate):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f'device {device}')\n",
    "\n",
    "    cnt_model = model.count\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = 0.0\n",
    "    cnt_train_data = len(train_data_loader)\n",
    "    cnt_valid_data = len(valid_data_loader)\n",
    "\n",
    "    chances = 2\n",
    "    remain_chances = chances\n",
    "\n",
    "    prev_valid_loss = float(\"inf\")\n",
    "\n",
    "    start = time.time_ns()\n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch: {epoch}')\n",
    "        for batch_idx, (input_data, target) in enumerate(train_data_loader, 0):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            outputs = model(input_data)\n",
    "            loss = loss_function(outputs, target)\n",
    "            train_losses += (loss.item() / cnt_model)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10000 == 0:\n",
    "                print(f'avg loss among models: {loss / cnt_model}')\n",
    "\n",
    "        train_loss = train_losses / cnt_train_data\n",
    "\n",
    "        corrects = [0.0] * cnt_model\n",
    "        valid_losses = 0.0\n",
    "        for batch_idx, (input_data, target) in enumerate(valid_data_loader, 0):\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(input_data)\n",
    "\n",
    "            loss = loss_function(output, target)\n",
    "            valid_losses += (loss.item() / cnt_model)\n",
    "\n",
    "            for i in range(cnt_model):\n",
    "                _, predicted = torch.max(output[i], 1) if cnt_model > 1 else torch.max(output, 1)\n",
    "\n",
    "                if predicted == target:\n",
    "                    corrects[i] += 1.0\n",
    "\n",
    "        valid_loss = valid_losses / cnt_valid_data\n",
    "\n",
    "        should_finish = False\n",
    "\n",
    "        valid_accuracies = (np.array(corrects) / cnt_valid_data)\n",
    "\n",
    "        max_valid_index = np.argmax(valid_accuracies)\n",
    "        max_valid_acc = np.max(valid_accuracies)\n",
    "\n",
    "        print(f'max valid accuracy from model #{max_valid_index}: {max_valid_acc * 100.0}')\n",
    "\n",
    "        if max_valid_acc > target_valid_accuracy:\n",
    "            should_finish = True\n",
    "\n",
    "        print(f'train loss: {train_loss}, valid loss: {valid_loss}')\n",
    "        if valid_loss > prev_valid_loss:\n",
    "            if remain_chances == 0:\n",
    "                should_finish = True\n",
    "            else:\n",
    "                remain_chances -= 1\n",
    "        else:\n",
    "            remain_chances = chances\n",
    "\n",
    "        prev_valid_loss = valid_loss\n",
    "\n",
    "        if should_finish:\n",
    "            end = time.time_ns()\n",
    "            print(f'stop training')\n",
    "            best_accuracy = max_valid_acc * 100.0\n",
    "            print(f'achieved best valid accuracy: {best_accuracy}%')\n",
    "            print(f'executed epochs: {epoch}')\n",
    "            elapsed_time = (end - start) / 1000_000_000.0\n",
    "            print(f'elapsed training time {elapsed_time} seconds')\n",
    "\n",
    "            with open(f'{type(model).__name__}.txt', 'w') as f:\n",
    "                f.write(f'{epoch + 1} {elapsed_time} {target_valid_accuracy} {best_accuracy}')\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_root = '../data/cifar-10'\n",
    "batch_size = 1\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                         std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(root=data_root,\n",
    "                              train=True,\n",
    "                              transform=data_transforms,\n",
    "                              download=True)\n",
    "train_data_loader = DataLoader(train_data,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=False)\n",
    "\n",
    "\n",
    "test_data = datasets.CIFAR10(root=data_root,\n",
    "                             train=False,\n",
    "                             transform=data_transforms,\n",
    "                             download=True)\n",
    "test_data_loader = DataLoader(test_data,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "\n",
    "    plt.imshow(np.transpose(npimg.reshape(3,32,32), (1,2,0)).copy())\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# training configuration\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "target_valid_accuracy = 0.4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "epoch: 0\n",
      "avg loss among models: 2.049748659133911\n",
      "avg loss among models: 1.8854020833969116\n",
      "avg loss among models: 2.191504955291748\n",
      "avg loss among models: 2.334974765777588\n",
      "avg loss among models: 2.2918152809143066\n",
      "max valid accuracy from model #0: 61.839999999999996\n",
      "train loss: 1.8344030031967162, valid loss: 1.7514836047172546\n",
      "stop training\n",
      "achieved best valid accuracy: 61.839999999999996%\n",
      "executed epochs: 0\n",
      "elapsed training time 178.910313789 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yskim/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py:2610: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train(model=MultiModel_2(),\n",
    "      loss_function=OnlineDistillationLoss(),\n",
    "      train_data_loader=train_data_loader,\n",
    "      valid_data_loader=test_data_loader,\n",
    "      target_valid_accuracy=target_valid_accuracy,\n",
    "      epochs=epochs,\n",
    "      learning_rate=lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "epoch: 0\n",
      "avg loss among models: 2.075756549835205\n",
      "avg loss among models: 1.89504075050354\n",
      "avg loss among models: 2.2569732666015625\n",
      "avg loss among models: 2.3031485080718994\n",
      "avg loss among models: 1.9140231609344482\n",
      "accuracy of model #0: 59.95%\n",
      "accuracy of model #1: 58.96%\n",
      "accuracy of model #2: 60.01%\n",
      "accuracy of model #3: 60.61%\n",
      "epoch: 1\n",
      "avg loss among models: 1.6761380434036255\n",
      "avg loss among models: 1.8591288328170776\n",
      "avg loss among models: 2.3160436153411865\n",
      "avg loss among models: 2.3319597244262695\n",
      "avg loss among models: 1.6956121921539307\n",
      "accuracy of model #0: 65.62%\n",
      "accuracy of model #1: 66.25999999999999%\n",
      "accuracy of model #2: 65.8%\n",
      "accuracy of model #3: 65.29%\n",
      "epoch: 2\n",
      "avg loss among models: 1.5011800527572632\n",
      "avg loss among models: 1.647318959236145\n",
      "avg loss among models: 2.3073654174804688\n",
      "avg loss among models: 2.3466482162475586\n",
      "avg loss among models: 1.6553175449371338\n",
      "accuracy of model #0: 68.39%\n",
      "accuracy of model #1: 67.99%\n",
      "accuracy of model #2: 68.67%\n",
      "accuracy of model #3: 67.36%\n",
      "epoch: 3\n",
      "avg loss among models: 1.5471796989440918\n",
      "avg loss among models: 1.754246473312378\n",
      "avg loss among models: 1.8897465467453003\n",
      "avg loss among models: 2.4207043647766113\n",
      "avg loss among models: 1.3707070350646973\n",
      "accuracy of model #0: 68.99%\n",
      "accuracy of model #1: 68.44%\n",
      "accuracy of model #2: 68.8%\n",
      "accuracy of model #3: 69.39999999999999%\n",
      "epoch: 4\n",
      "avg loss among models: 1.4013878107070923\n",
      "avg loss among models: 1.3623055219650269\n",
      "avg loss among models: 1.8945109844207764\n",
      "avg loss among models: 2.4108128547668457\n",
      "avg loss among models: 1.379813313484192\n",
      "accuracy of model #0: 69.12%\n",
      "accuracy of model #1: 68.81%\n",
      "accuracy of model #2: 70.06%\n",
      "accuracy of model #3: 69.89999999999999%\n",
      "achieve target valid accuracy: 70.0%, stop training\n",
      "executed epochs: 4\n",
      "elapsed training time 1520.469780569 seconds\n"
     ]
    }
   ],
   "source": [
    "train(model=MultiModel_4(),\n",
    "      loss_function=OnlineDistillationLoss(),\n",
    "      train_data_loader=train_data_loader,\n",
    "      valid_data_loader=test_data_loader,\n",
    "      target_valid_accuracy=target_valid_accuracy,\n",
    "      epochs=epochs,\n",
    "      learning_rate=lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "epoch: 0\n",
      "avg loss among models: 2.081385374069214\n",
      "avg loss among models: 1.7985424995422363\n",
      "avg loss among models: 2.2642011642456055\n",
      "avg loss among models: 2.3326854705810547\n",
      "avg loss among models: 2.1625137329101562\n",
      "accuracy of model #0: 59.12%\n",
      "accuracy of model #1: 61.14000000000001%\n",
      "accuracy of model #2: 61.29%\n",
      "accuracy of model #3: 61.08%\n",
      "accuracy of model #4: 60.650000000000006%\n",
      "accuracy of model #5: 59.760000000000005%\n",
      "accuracy of model #6: 60.34%\n",
      "accuracy of model #7: 60.660000000000004%\n",
      "epoch: 1\n",
      "avg loss among models: 1.696842908859253\n",
      "avg loss among models: 1.652596354484558\n",
      "avg loss among models: 2.3210926055908203\n",
      "avg loss among models: 2.3656201362609863\n",
      "avg loss among models: 1.499781608581543\n",
      "accuracy of model #0: 66.11%\n",
      "accuracy of model #1: 66.96%\n",
      "accuracy of model #2: 66.99000000000001%\n",
      "accuracy of model #3: 65.63%\n",
      "accuracy of model #4: 65.47%\n",
      "accuracy of model #5: 65.5%\n",
      "accuracy of model #6: 65.3%\n",
      "accuracy of model #7: 66.17%\n",
      "epoch: 2\n",
      "avg loss among models: 1.4248378276824951\n",
      "avg loss among models: 1.3733501434326172\n",
      "avg loss among models: 2.3294503688812256\n",
      "avg loss among models: 2.3710670471191406\n",
      "avg loss among models: 1.4022395610809326\n",
      "accuracy of model #0: 68.91000000000001%\n",
      "accuracy of model #1: 67.89%\n",
      "accuracy of model #2: 68.81%\n",
      "accuracy of model #3: 67.33%\n",
      "accuracy of model #4: 67.32000000000001%\n",
      "accuracy of model #5: 68.60000000000001%\n",
      "accuracy of model #6: 67.78%\n",
      "accuracy of model #7: 67.5%\n",
      "epoch: 3\n",
      "avg loss among models: 1.473594307899475\n",
      "avg loss among models: 1.3645349740982056\n",
      "avg loss among models: 2.1843769550323486\n",
      "avg loss among models: 2.346238136291504\n",
      "avg loss among models: 1.3651467561721802\n",
      "accuracy of model #0: 69.01%\n",
      "accuracy of model #1: 69.39999999999999%\n",
      "accuracy of model #2: 69.53%\n",
      "accuracy of model #3: 68.45%\n",
      "accuracy of model #4: 69.08999999999999%\n",
      "accuracy of model #5: 69.27%\n",
      "accuracy of model #6: 69.92%\n",
      "accuracy of model #7: 69.08999999999999%\n",
      "epoch: 4\n",
      "avg loss among models: 1.4142966270446777\n",
      "avg loss among models: 1.538633108139038\n",
      "avg loss among models: 2.074770212173462\n",
      "avg loss among models: 2.367525815963745\n",
      "avg loss among models: 1.5100674629211426\n",
      "accuracy of model #0: 70.08%\n",
      "accuracy of model #1: 69.94%\n",
      "accuracy of model #2: 70.35%\n",
      "accuracy of model #3: 69.59%\n",
      "accuracy of model #4: 69.33%\n",
      "accuracy of model #5: 68.99%\n",
      "accuracy of model #6: 69.21000000000001%\n",
      "accuracy of model #7: 69.05%\n",
      "achieve target valid accuracy: 70.0%, stop training\n",
      "executed epochs: 4\n",
      "elapsed training time 2881.038124402 seconds\n"
     ]
    }
   ],
   "source": [
    "train(model=MultiModel_8(),\n",
    "      loss_function=OnlineDistillationLoss(),\n",
    "      train_data_loader=train_data_loader,\n",
    "      valid_data_loader=test_data_loader,\n",
    "      target_valid_accuracy=target_valid_accuracy,\n",
    "      epochs=epochs,\n",
    "      learning_rate=lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "epoch: 0\n",
      "avg loss among models: 2.3024001121520996\n",
      "avg loss among models: 2.4309301376342773\n",
      "avg loss among models: 2.461027145385742\n",
      "avg loss among models: 2.460101842880249\n",
      "avg loss among models: 2.460869789123535\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 5.80 GiB total capacity; 3.05 GiB already allocated; 259.62 MiB free; 3.06 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-c5a29cfbe1b6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m train(model=SingleModel(),\n\u001B[0m\u001B[1;32m      2\u001B[0m       \u001B[0mloss_function\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCrossEntropyLoss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m       \u001B[0mtrain_data_loader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrain_data_loader\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m       \u001B[0mvalid_data_loader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtest_data_loader\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m       \u001B[0mtarget_valid_accuracy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtarget_valid_accuracy\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-26e5df9ab270>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(model, loss_function, train_data_loader, valid_data_loader, target_valid_accuracy, epochs, learning_rate)\u001B[0m\n\u001B[1;32m     77\u001B[0m             \u001B[0mtarget\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 79\u001B[0;31m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     80\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     81\u001B[0m             \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloss_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-4-ad06728d1380>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv2d_0\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu_0\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax_pool2d_0\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    397\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    398\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 399\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conv_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    400\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mConv3d\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_ConvNd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001B[0m in \u001B[0;36m_conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    393\u001B[0m                             \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstride\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    394\u001B[0m                             _pair(0), self.dilation, self.groups)\n\u001B[0;32m--> 395\u001B[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001B[0m\u001B[1;32m    396\u001B[0m                         self.padding, self.dilation, self.groups)\n\u001B[1;32m    397\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 5.80 GiB total capacity; 3.05 GiB already allocated; 259.62 MiB free; 3.06 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "train(model=SingleModel(),\n",
    "      loss_function=nn.CrossEntropyLoss(),\n",
    "      train_data_loader=train_data_loader,\n",
    "      valid_data_loader=test_data_loader,\n",
    "      target_valid_accuracy=target_valid_accuracy,\n",
    "      epochs=epochs,\n",
    "      learning_rate=lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import torch.onnx as onnx\n",
    "\n",
    "model = MultiModel()\n",
    "model.eval()\n",
    "\n",
    "x = torch.rand(1, 3, 32,  32, requires_grad=True)\n",
    "out = model(x)\n",
    "\n",
    "onnx.export(model,\n",
    "            x,\n",
    "            \"multi_custom_model.onnx\",\n",
    "            export_params=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output_0', 'output_1'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def train_multi_model():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    use_cuda = True if torch.cuda.is_available() else False\n",
    "    print(f'device {device}')\n",
    "\n",
    "    model = MultiModel().to(device)\n",
    "\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    kld_loss = nn.KLDivLoss(reduction='mean')\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch: {epoch}')\n",
    "        for batch_idx, (input_data, target) in enumerate(train_data_loader, 0):\n",
    "            if batch_idx >= 1:\n",
    "                break\n",
    "\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            with profiler.profile(record_shapes=True,\n",
    "                                  profile_memory=True,\n",
    "                                  use_cuda=use_cuda) as prof:\n",
    "                with profiler.record_function(\"train_multi_custom_model\"):\n",
    "                    output_0, output_1 = model(input_data)\n",
    "\n",
    "                    loss_0 = ce_loss(output_0, target) + kld_loss(output_1.detach().clone(), output_0)\n",
    "                    loss_1 = ce_loss(output_1, target) + kld_loss(output_0.detach().clone(), output_1)\n",
    "\n",
    "                    loss = loss_0 + loss_1\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            if batch_idx % 10000 == 0:\n",
    "                print(f'{loss_0.item()}, {loss_1.item()}')\n",
    "\n",
    "            print(prof.key_averages())\n",
    "            prof.export_chrome_trace(\"trace_multi_custom_model.json\")\n",
    "\n",
    "    correct_0 = 0\n",
    "    correct_1 = 0\n",
    "    total = len(test_data_loader)\n",
    "    for batch_idx, (input_data, target) in enumerate(test_data_loader, 0):\n",
    "        input_data = input_data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output_0, output_1 = model(input_data)\n",
    "        _, predicted = torch.max(output_0, 1)\n",
    "        if predicted == target:\n",
    "            correct_0 += 1\n",
    "\n",
    "        _, predicted = torch.max(output_1, 1)\n",
    "        if predicted == target:\n",
    "            correct_1 += 1\n",
    "\n",
    "    print(correct_0 / total * 100.0)\n",
    "    print(correct_1 / total * 100.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "epoch: 0\n",
      "2.0938072204589844, 2.027334690093994\n",
      "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                   [memory]         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     -16.08 Mb     -16.08 Mb            24  \n",
      "                                aten::zeros         0.17%      24.560us         0.30%      43.288us      14.429us      37.314us         0.26%      43.233us      14.411us          12 b           0 b           0 b           0 b             3  \n",
      "                                aten::empty         2.51%     362.313us         2.51%     362.313us       2.970us       0.000us         0.00%       0.000us       0.000us          60 b          60 b      17.45 Mb      17.45 Mb           122  \n",
      "                                aten::zero_         0.24%      34.440us         0.52%      74.735us       8.304us      31.744us         0.22%      67.872us       7.541us           0 b           0 b           0 b           0 b             9  \n",
      "                   train_multi_custom_model        44.17%       6.381ms        72.89%      10.531ms      10.531ms       6.272ms        42.99%      10.578ms      10.578ms          -4 b         -20 b       1.14 Mb      -4.50 Kb             1  \n",
      "                               aten::conv2d         0.19%      27.543us         3.59%     518.708us     129.677us      21.952us         0.15%     488.480us     122.120us           0 b           0 b     384.00 Kb           0 b             4  \n",
      "                          aten::convolution         0.18%      25.781us         3.40%     491.165us     122.791us      19.488us         0.13%     466.528us     116.632us           0 b           0 b     384.00 Kb           0 b             4  \n",
      "                         aten::_convolution         0.43%      61.933us         3.22%     465.384us     116.346us      42.048us         0.29%     447.040us     111.760us           0 b           0 b     384.00 Kb           0 b             4  \n",
      "                    aten::cudnn_convolution         1.94%     280.978us         2.03%     292.686us      73.171us     340.416us         2.33%     340.416us      85.104us           0 b           0 b     384.00 Kb     384.00 Kb             4  \n",
      "                              aten::resize_         0.55%      78.953us         0.55%      78.953us       1.974us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     295.00 Kb     295.00 Kb            40  \n",
      "                              aten::reshape         0.25%      35.803us         0.54%      77.844us       7.784us      46.176us         0.32%      46.176us       4.618us           0 b           0 b           0 b           0 b            10  \n",
      "                                 aten::view         0.44%      63.679us         0.44%      63.679us       3.184us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            20  \n",
      "                                 aten::add_         4.59%     663.392us         4.59%     663.392us      22.113us     713.922us         4.89%     713.922us      23.797us           0 b           0 b           0 b           0 b            30  \n",
      "                                 aten::relu         0.58%      83.996us         1.24%     178.672us      29.779us      62.240us         0.43%     141.024us      23.504us           0 b           0 b     388.00 Kb           0 b             6  \n",
      "                            aten::threshold         0.45%      65.511us         0.66%      94.676us      15.779us      78.784us         0.54%      78.784us      13.131us           0 b           0 b     388.00 Kb           0 b             6  \n",
      "                           aten::max_pool2d         0.18%      25.452us         1.12%     161.758us      40.439us      23.840us         0.16%     159.712us      39.928us           0 b           0 b     288.00 Kb           0 b             4  \n",
      "              aten::max_pool2d_with_indices         0.68%      98.237us         0.94%     136.306us      34.076us     135.872us         0.93%     135.872us      33.968us           0 b           0 b     288.00 Kb           0 b             4  \n",
      "                                  aten::add         0.90%     130.266us         0.90%     130.266us      18.609us     116.512us         0.80%     116.512us      16.645us           0 b           0 b       3.50 Kb       3.50 Kb             7  \n",
      "                           aten::batch_norm         0.17%      24.312us         1.84%     265.439us      66.360us      21.344us         0.15%     262.976us      65.744us           0 b           0 b     100.00 Kb           0 b             4  \n",
      "               aten::_batch_norm_impl_index         0.18%      25.927us         1.67%     241.127us      60.282us      26.240us         0.18%     241.632us      60.408us           0 b           0 b     100.00 Kb           0 b             4  \n",
      "                     aten::cudnn_batch_norm         1.03%     148.843us         1.49%     215.200us      53.800us     215.392us         1.48%     215.392us      53.848us           0 b           0 b     100.00 Kb           0 b             4  \n",
      "                           aten::empty_like         0.31%      44.660us         0.85%     122.699us       6.458us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     485.50 Kb           0 b            19  \n",
      "                              aten::flatten         0.07%       9.689us         0.14%      19.735us       9.868us      18.976us         0.13%      18.976us       9.488us           0 b           0 b           0 b           0 b             2  \n",
      "                               aten::linear         0.21%      29.747us         1.95%     282.115us      70.529us      60.992us         0.42%     472.960us     118.240us           0 b           0 b       5.00 Kb           0 b             4  \n",
      "                                    aten::t         0.45%      65.277us         0.83%     119.454us       5.973us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            20  \n",
      "                            aten::transpose         0.25%      35.572us         0.38%      54.177us       2.709us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            20  \n",
      "                           aten::as_strided         0.16%      23.591us         0.16%      23.591us       0.907us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            26  \n",
      "                                aten::addmm         1.26%     182.163us         1.50%     217.011us      54.253us     411.968us         2.82%     411.968us     102.992us           0 b           0 b       5.00 Kb           0 b             4  \n",
      "                               aten::expand         0.06%       8.271us         0.08%      11.366us       2.842us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             4  \n",
      "                              aten::softmax         0.10%      13.784us         0.51%      74.232us      37.116us       6.144us         0.04%      37.088us      18.544us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                             aten::_softmax         0.30%      43.040us         0.42%      60.448us      30.224us      30.944us         0.21%      30.944us      15.472us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                          aten::log_softmax         0.08%      11.701us         0.40%      57.980us      28.990us       8.480us         0.06%      38.240us      19.120us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                         aten::_log_softmax         0.23%      33.685us         0.32%      46.279us      23.139us      29.760us         0.20%      29.760us      14.880us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                             aten::nll_loss         0.08%      11.093us         0.43%      61.488us      30.744us      10.432us         0.07%      60.865us      30.432us           0 b           0 b       2.00 Kb           0 b             2  \n",
      "                     aten::nll_loss_forward         0.35%      50.395us         0.35%      50.395us      25.197us      50.432us         0.35%      50.432us      25.216us           0 b           0 b       2.00 Kb       2.00 Kb             2  \n",
      "                               aten::detach         0.07%       9.744us         0.10%      15.149us       7.575us       9.152us         0.06%      14.272us       7.136us           0 b           0 b           0 b           0 b             2  \n",
      "                                     detach         0.04%       5.405us         0.04%       5.405us       2.702us       5.120us         0.04%       5.120us       2.560us           0 b           0 b           0 b           0 b             2  \n",
      "                                aten::clone         0.17%      25.066us         0.40%      58.331us      29.166us      34.369us         0.24%      58.688us      29.344us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                        aten::empty_strided         0.83%     119.995us         0.83%     119.995us       4.444us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      16.09 Mb      16.09 Mb            27  \n",
      "                                aten::copy_         2.42%     349.901us         2.42%     349.901us      13.458us     419.359us         2.87%     419.359us      16.129us           0 b           0 b           0 b           0 b            26  \n",
      "                               aten::kl_div         3.59%     518.981us        12.67%       1.831ms     915.364us       1.023ms         7.01%       1.892ms     945.776us           0 b           0 b       1.00 Kb      -6.00 Kb             2  \n",
      "                                  aten::log         2.99%     432.059us         3.13%     452.348us     113.087us     516.608us         3.54%     516.608us     129.152us           0 b           0 b       2.00 Kb           0 b             4  \n",
      "                                  aten::sub         0.13%      18.401us         0.18%      26.694us      13.347us      17.472us         0.12%      17.472us       8.736us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                                  aten::mul         0.44%      63.624us         0.59%      85.874us      14.312us      78.112us         0.54%      78.112us      13.019us           0 b           0 b       3.00 Kb           0 b             6  \n",
      "                           aten::zeros_like         0.22%      31.350us         0.92%     133.331us      22.222us      56.866us         0.39%     118.819us      19.803us           0 b           0 b     385.00 Kb           0 b             6  \n",
      "                                aten::fill_         0.33%      47.685us         0.33%      47.685us       6.812us      43.872us         0.30%      43.872us       6.267us           0 b           0 b           0 b           0 b             7  \n",
      "                                   aten::gt         0.35%      50.029us         0.82%     118.337us      29.584us      42.944us         0.29%      84.544us      21.136us           0 b           0 b       2.00 Kb           0 b             4  \n",
      "                                 aten::item         0.11%      16.576us         0.17%      24.319us       6.080us      14.079us         0.10%      21.919us       5.480us           0 b           0 b           0 b           0 b             4  \n",
      "                  aten::_local_scalar_dense         0.05%       7.743us         0.05%       7.743us       1.936us       7.840us         0.05%       7.840us       1.960us           0 b           0 b           0 b           0 b             4  \n",
      "                                aten::where         0.08%      11.956us         2.69%     388.664us     194.332us      10.720us         0.07%      26.944us      13.472us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                             aten::_s_where         2.55%     369.102us         2.61%     376.708us     188.354us      16.224us         0.11%      16.224us       8.112us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                                 aten::mean         3.68%     531.001us         3.74%     539.708us     269.854us     446.112us         3.06%     446.112us     223.056us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "          Optimizer.zero_grad#SGD.zero_grad         0.18%      26.135us         0.20%      29.108us      29.108us      29.152us         0.20%      29.152us      29.152us          -4 b         -20 b           0 b           0 b             1  \n",
      "                            aten::ones_like         0.04%       6.403us         0.15%      21.274us      21.274us      13.695us         0.09%      21.439us      21.439us           0 b           0 b         512 b           0 b             1  \n",
      "                               AddBackward0         0.24%      34.037us         0.24%      34.037us       4.862us      31.967us         0.22%      31.967us       4.567us           0 b           0 b           0 b           0 b             7  \n",
      "                              KlDivBackward         0.46%      67.139us        11.70%       1.690ms     844.858us     127.393us         0.87%       1.690ms     844.944us           0 b           0 b       1.00 Kb      -2.00 Kb             2  \n",
      "                                 aten::sub_         0.16%      22.939us         0.16%      22.939us      11.469us      21.472us         0.15%      21.472us      10.736us           0 b           0 b           0 b           0 b             2  \n",
      "                                   aten::eq         0.39%      56.581us         0.88%     127.242us      31.811us      65.824us         0.45%     127.041us      31.760us           0 b           0 b       2.00 Kb           0 b             4  \n",
      "                         aten::masked_fill_         4.77%     688.570us         4.77%     688.570us     344.285us     687.776us         4.71%     687.776us     343.888us           0 b           0 b           0 b           0 b             2  \n",
      "                                 aten::div_         1.38%     199.797us         1.38%     199.797us      99.898us     174.560us         1.20%     174.560us      87.280us           0 b           0 b           0 b           0 b             2  \n",
      "                            NllLossBackward         0.11%      15.251us         0.39%      56.042us      28.021us      14.655us         0.10%      54.879us      27.440us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                    aten::nll_loss_backward         0.28%      40.791us         0.28%      40.791us      20.396us      40.224us         0.28%      40.224us      20.112us           0 b           0 b       1.00 Kb       1.00 Kb             2  \n",
      "                         LogSoftmaxBackward         0.09%      12.752us         0.35%      50.123us      25.061us      12.449us         0.09%      49.312us      24.656us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "           aten::_log_softmax_backward_data         0.17%      25.000us         0.26%      37.371us      18.686us      36.864us         0.25%      36.864us      18.432us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                            SoftmaxBackward         0.10%      14.703us         0.58%      83.670us      41.835us      12.287us         0.08%      78.560us      39.280us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "               aten::_softmax_backward_data         0.24%      33.958us         0.48%      68.967us      34.484us      41.472us         0.28%      66.272us      33.136us           0 b           0 b       1.00 Kb      -1.00 Kb             2  \n",
      "                              AddmmBackward         0.48%      69.361us         2.43%     350.756us      87.689us     108.163us         0.74%     438.403us     109.601us           0 b           0 b      15.70 Mb           0 b             4  \n",
      "                                 aten::conj         0.14%      20.525us         0.14%      20.525us       2.566us      16.352us         0.11%      16.352us       2.044us           0 b           0 b           0 b           0 b             8  \n",
      "                                   aten::mm         1.16%     167.069us         1.37%     197.430us      24.679us     313.888us         2.15%     313.888us      39.236us           0 b           0 b      15.70 Mb           0 b             8  \n",
      "            torch::autograd::AccumulateGrad         1.28%     185.572us         4.86%     701.577us      29.232us     155.651us         1.07%     701.475us      29.228us           0 b           0 b      16.08 Mb           0 b            24  \n",
      "                    aten::new_empty_strided         0.58%      84.302us         1.31%     188.790us       7.866us     150.785us         1.03%     150.785us       6.283us           0 b           0 b      16.08 Mb           0 b            24  \n",
      "                                  TBackward         0.08%      12.071us         0.23%      32.728us       8.182us      21.153us         0.15%      21.153us       5.288us           0 b           0 b           0 b           0 b             4  \n",
      "                              ReluBackward0         0.24%      34.705us         0.98%     141.943us      23.657us      31.840us         0.22%     139.423us      23.237us           0 b           0 b     388.00 Kb           0 b             6  \n",
      "                   aten::threshold_backward         0.56%      80.976us         0.74%     107.238us      17.873us     107.583us         0.74%     107.583us      17.930us           0 b           0 b     388.00 Kb           0 b             6  \n",
      "                               ViewBackward         0.22%      31.288us         0.50%      72.320us      12.053us      23.677us         0.16%      55.581us       9.264us           0 b           0 b           0 b           0 b             6  \n",
      "                     CudnnBatchNormBackward         0.24%      35.319us         1.27%     182.970us      45.743us      20.033us         0.14%     119.520us      29.880us           0 b           0 b     100.00 Kb           0 b             4  \n",
      "            aten::cudnn_batch_norm_backward         0.70%     101.098us         1.02%     147.651us      36.913us      99.487us         0.68%      99.487us      24.872us           0 b           0 b     100.00 Kb           0 b             4  \n",
      "               MaxPool2DWithIndicesBackward         0.20%      28.544us         1.38%     199.350us      49.838us      23.682us         0.16%     199.809us      49.952us           0 b           0 b     384.00 Kb           0 b             4  \n",
      "     aten::max_pool2d_with_indices_backward         0.47%      67.916us         1.18%     170.806us      42.701us      72.124us         0.49%     176.127us      44.032us           0 b           0 b     384.00 Kb           0 b             4  \n",
      "                           aten::resize_as_         0.09%      12.677us         0.10%      14.667us       3.667us      15.296us         0.10%      15.296us       3.824us           0 b           0 b           0 b           0 b             4  \n",
      "                   CudnnConvolutionBackward         0.22%      31.100us         2.10%     303.283us      75.821us      26.688us         0.18%     412.575us     103.144us           0 b           0 b     483.00 Kb           0 b             4  \n",
      "           aten::cudnn_convolution_backward         0.37%      52.963us         1.88%     272.183us      68.046us      37.278us         0.26%     385.888us      96.472us           0 b           0 b     483.00 Kb           0 b             4  \n",
      "     aten::cudnn_convolution_backward_input         0.53%      76.951us         0.56%      81.268us      40.634us     145.889us         1.00%     145.889us      72.944us           0 b           0 b      64.00 Kb      64.00 Kb             2  \n",
      "    aten::cudnn_convolution_backward_weight         0.80%     115.746us         0.95%     137.952us      34.488us     202.721us         1.39%     202.721us      50.680us           0 b           0 b     419.00 Kb           0 b             4  \n",
      "                    Optimizer.step#SGD.step         1.25%     180.081us         2.87%     414.800us     414.800us     131.103us         0.90%     468.353us     468.353us          -4 b         -20 b           0 b           0 b             1  \n",
      "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 14.447ms\n",
      "Self CUDA time total: 14.588ms\n",
      "\n",
      "9.62\n",
      "11.379999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yskim/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py:2610: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#train_multi_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_resnet18():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f'device {device}')\n",
    "\n",
    "    model = models.resnet18()\n",
    "    model.fc = nn.Linear(512, 10)\n",
    "    model = model.to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch: {epoch}')\n",
    "        for batch_idx, (input_data, target) in enumerate(train_data_loader, 0):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(input_data)\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10000 == 0:\n",
    "                print(loss)\n",
    "\n",
    "    correct = 0\n",
    "    total = len(test_data_loader)\n",
    "    for batch_idx, (input_data, target) in enumerate(test_data_loader, 0):\n",
    "        input_data = input_data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(input_data)\n",
    "\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "        if predicted == target:\n",
    "            correct += 1\n",
    "    print(correct / total * 100.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#train_resnet18()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}