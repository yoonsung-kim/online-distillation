{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import models, datasets, transforms\n",
    "\n",
    "from test.models import *\n",
    "from utils.losses import OnlineDistillationLoss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def test_train(config):\n",
    "    #device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = config['device']\n",
    "    print(f'device {device}')\n",
    "\n",
    "    model = config['model']\n",
    "    cnt_model = model.count\n",
    "    print(f'model count: {cnt_model}')\n",
    "    #model = model.to(device)\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # check Nimble availability and optimize a model\n",
    "    if config['use_nimble']:\n",
    "        dummy_input = torch.zeros(config['input_shape']).to(device)\n",
    "        model = torch.cuda.Nimble(model)\n",
    "        model.prepare(dummy_input, training=True)\n",
    "\n",
    "    train_data_loader = config['train_data_loader']\n",
    "\n",
    "    # instances for backpropagation & updating weights\n",
    "    optimizer = config['optimizer']\n",
    "    loss_function = config['loss_function']\n",
    "\n",
    "    print(f'test one iteration...')\n",
    "\n",
    "    million = 1000_000.0\n",
    "\n",
    "    dict = {\n",
    "        'elapsed_time': {\n",
    "            'unit': 'millisecond',\n",
    "            'forward': 0.0,\n",
    "            'loss_calculation': 0.0,\n",
    "            'backward': 0.0,\n",
    "            'set_gradients_zero': 0.0,\n",
    "            'update_weights': 0.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    elapsed_time = dict['elapsed_time']\n",
    "\n",
    "    for batch_idx, (input_data, target) in enumerate(train_data_loader, 0):\n",
    "\n",
    "        input_data = input_data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        start = time.time_ns()\n",
    "        outputs = model(input_data)\n",
    "        end = time.time_ns()\n",
    "        elapsed_time['forward'] = (end - start) / million\n",
    "\n",
    "        start = time.time_ns()\n",
    "        loss = loss_function(outputs, target)\n",
    "        end = time.time_ns()\n",
    "        elapsed_time['loss_calculation'] = (end - start) / million\n",
    "\n",
    "        start = time.time_ns()\n",
    "        optimizer.zero_grad()\n",
    "        end = time.time_ns()\n",
    "        elapsed_time['set_gradients_zero'] = (end - start) / million\n",
    "\n",
    "        start = time.time_ns()\n",
    "        loss.backward()\n",
    "        end = time.time_ns()\n",
    "        elapsed_time['backward'] = (end - start) / million\n",
    "\n",
    "        start = time.time_ns()\n",
    "        optimizer.step()\n",
    "        end = time.time_ns()\n",
    "        elapsed_time['update_weights'] = (end - start) / million\n",
    "\n",
    "        break\n",
    "\n",
    "    print(f'stop training')\n",
    "\n",
    "    with open(f'{config[\"output_file_path\"]}', 'w', encoding='utf-8') as f:\n",
    "        json.dump(dict, f, ensure_ascii=False)\n",
    "\n",
    "def usual_train(config):\n",
    "    #device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = config['device']\n",
    "    print(f'device {device}')\n",
    "\n",
    "    model = config['model']\n",
    "    cnt_model = model.count\n",
    "    print(f'model count: {cnt_model}')\n",
    "    #model = model.to(device)\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # check Nimble availability and optimize a model\n",
    "    if config['use_nimble']:\n",
    "        dummy_input = torch.zeros(config['input_shape']).to(device)\n",
    "        model = torch.cuda.Nimble(model)\n",
    "        model.prepare(dummy_input, training=True)\n",
    "\n",
    "    train_losses = 0.0\n",
    "\n",
    "    train_data_loader = config['train_data_loader']\n",
    "    cnt_train_data = len(train_data_loader)\n",
    "\n",
    "    valid_data_loader = config['valid_data_loader']\n",
    "    cnt_valid_data = len(valid_data_loader)\n",
    "\n",
    "    chances = 2\n",
    "    remain_chances = chances\n",
    "\n",
    "    prev_valid_loss = float(\"inf\")\n",
    "\n",
    "    # instances for backpropagation & updating weights\n",
    "    optimizer = config['optimizer']\n",
    "    loss_function = config['loss_function']\n",
    "\n",
    "    start = time.time_ns()\n",
    "    for epoch in range(config['epochs']):\n",
    "        print(f'epoch: {epoch}')\n",
    "        for batch_idx, (input_data, target) in enumerate(train_data_loader, 0):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            outputs = model(input_data)\n",
    "            loss = loss_function(outputs, target)\n",
    "            train_losses += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10000 == 0:\n",
    "                print(f'avg loss among models: {loss}')\n",
    "\n",
    "        train_loss = train_losses / cnt_train_data\n",
    "\n",
    "        corrects = [0.0] * cnt_model\n",
    "        valid_losses = 0.0\n",
    "\n",
    "        for batch_idx, (input_data, target) in enumerate(valid_data_loader, 0):\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(input_data)\n",
    "\n",
    "            loss = loss_function(output, target)\n",
    "            valid_losses += (loss.item() / cnt_model)\n",
    "\n",
    "            for i in range(cnt_model):\n",
    "                _, predicted = torch.max(output[i], 1) if cnt_model > 1 else torch.max(output, 1)\n",
    "\n",
    "                if predicted == target:\n",
    "                    corrects[i] += 1.0\n",
    "\n",
    "        valid_loss = valid_losses / cnt_valid_data\n",
    "\n",
    "        should_finish = False\n",
    "\n",
    "        valid_accuracies = (np.array(corrects) / cnt_valid_data)\n",
    "\n",
    "        max_valid_index = np.argmax(valid_accuracies)\n",
    "        max_valid_acc = np.max(valid_accuracies)\n",
    "\n",
    "        print(f'max valid accuracy from model #{max_valid_index}: {max_valid_acc * 100.0}')\n",
    "\n",
    "        if max_valid_acc > target_valid_accuracy:\n",
    "            should_finish = True\n",
    "\n",
    "        print(f'train loss: {train_loss}, valid loss: {valid_loss}')\n",
    "        if valid_loss > prev_valid_loss:\n",
    "            if remain_chances == 0:\n",
    "                should_finish = True\n",
    "            else:\n",
    "                remain_chances -= 1\n",
    "        else:\n",
    "            remain_chances = chances\n",
    "\n",
    "        prev_valid_loss = valid_loss\n",
    "\n",
    "        if should_finish:\n",
    "            end = time.time_ns()\n",
    "            print(f'stop training')\n",
    "            best_accuracy = max_valid_acc * 100.0\n",
    "            print(f'achieved best valid accuracy: {best_accuracy}%')\n",
    "            print(f'executed epochs: {epoch}')\n",
    "            elapsed_time = (end - start) / 1000_000_000.0\n",
    "            print(f'ett (elapsed training time)')\n",
    "            print(f'total ett: {elapsed_time} seconds')\n",
    "            print(f'avg ett: {elapsed_time / float(epochs + 1)} seconds')\n",
    "\n",
    "            with open(f'{config[\"output_file_path\"]}', 'w') as f:\n",
    "                f.write(f'{epoch + 1} {elapsed_time} {elapsed_time / float(epoch + 1)} {target_valid_accuracy} {best_accuracy}')\n",
    "            break\n",
    "\n",
    "def train(config,\n",
    "          test_one_iter=False):\n",
    "    if test_one_iter:\n",
    "        test_train(config)\n",
    "    else:\n",
    "        usual_train(config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "device cuda\n",
      "model count: 2\n",
      "test one iteration...\n",
      "stop training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yskim/miniconda3/envs/nimble/lib/python3.7/site-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    }
   ],
   "source": [
    "data_root = '../data/cifar-10'\n",
    "batch_size = 1\n",
    "input_width = 32\n",
    "input_height = input_width\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_width, input_height)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                         std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(root=data_root,\n",
    "                              train=True,\n",
    "                              transform=data_transforms,\n",
    "                              download=True)\n",
    "train_data_loader = DataLoader(train_data,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=False)\n",
    "\n",
    "\n",
    "test_data = datasets.CIFAR10(root=data_root,\n",
    "                             train=False,\n",
    "                             transform=data_transforms,\n",
    "                             download=True)\n",
    "test_data_loader = DataLoader(test_data,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=False)\n",
    "\n",
    "\n",
    "# training configuration\n",
    "lr = 0.01\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = MultiModel_2().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "loss_function = OnlineDistillationLoss()\n",
    "\n",
    "train_config = {\n",
    "    'device': device,\n",
    "    'model': model,\n",
    "    'optimizer': optimizer,\n",
    "    'loss_function': loss_function,\n",
    "    'epochs': 100,\n",
    "    'output_file_path': 'test_multi_model.json',\n",
    "    'use_nimble': True,\n",
    "    'input_shape': (batch_size, 3, input_height, input_width),\n",
    "    'train_data_loader': train_data_loader\n",
    "}\n",
    "\n",
    "train(train_config, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(device,\n",
    "          model,\n",
    "          loss_function,\n",
    "          optimizer,\n",
    "          train_data_loader,\n",
    "          valid_data_loader,\n",
    "          target_valid_accuracy,\n",
    "          epochs,\n",
    "          #learning_rate,\n",
    "          input_shape,\n",
    "          use_nimble,\n",
    "          output_file_path,\n",
    "          iters_per_epoch=None):\n",
    "    #device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f'device {device}')\n",
    "\n",
    "    cnt_model = model.count\n",
    "    print(f'model count: {cnt_model}')\n",
    "    #model = model.to(device)\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    #\n",
    "    if use_nimble:\n",
    "        dummy_input = torch.zeros(input_shape).to(device)\n",
    "        model = torch.cuda.Nimble(model)\n",
    "        model.prepare(dummy_input, training=True)\n",
    "    #\n",
    "\n",
    "    train_losses = 0.0\n",
    "    cnt_train_data = len(train_data_loader)\n",
    "    cnt_valid_data = len(valid_data_loader)\n",
    "\n",
    "    chances = 2\n",
    "    remain_chances = chances\n",
    "\n",
    "    prev_valid_loss = float(\"inf\")\n",
    "\n",
    "    start = time.time_ns()\n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch: {epoch}')\n",
    "        for batch_idx, (input_data, target) in enumerate(train_data_loader, 0):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            outputs = model(input_data)\n",
    "            loss = loss_function(outputs, target)\n",
    "            train_losses += loss.item()#(loss.item() / cnt_model)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10000 == 0:\n",
    "                print(f'avg loss among models: {loss}')\n",
    "\n",
    "        train_loss = train_losses / cnt_train_data\n",
    "\n",
    "        corrects = [0.0] * cnt_model\n",
    "        valid_losses = 0.0\n",
    "\n",
    "        for batch_idx, (input_data, target) in enumerate(valid_data_loader, 0):\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(input_data)\n",
    "\n",
    "            loss = loss_function(output, target)\n",
    "            valid_losses += (loss.item() / cnt_model)\n",
    "\n",
    "            for i in range(cnt_model):\n",
    "                _, predicted = torch.max(output[i], 1) if cnt_model > 1 else torch.max(output, 1)\n",
    "\n",
    "                if predicted == target:\n",
    "                    corrects[i] += 1.0\n",
    "\n",
    "        valid_loss = valid_losses / cnt_valid_data\n",
    "\n",
    "        should_finish = False\n",
    "\n",
    "        valid_accuracies = (np.array(corrects) / cnt_valid_data)\n",
    "\n",
    "        max_valid_index = np.argmax(valid_accuracies)\n",
    "        max_valid_acc = np.max(valid_accuracies)\n",
    "\n",
    "        print(f'max valid accuracy from model #{max_valid_index}: {max_valid_acc * 100.0}')\n",
    "\n",
    "        if max_valid_acc > target_valid_accuracy:\n",
    "            should_finish = True\n",
    "\n",
    "        print(f'train loss: {train_loss}, valid loss: {valid_loss}')\n",
    "        if valid_loss > prev_valid_loss:\n",
    "            if remain_chances == 0:\n",
    "                should_finish = True\n",
    "            else:\n",
    "                remain_chances -= 1\n",
    "        else:\n",
    "            remain_chances = chances\n",
    "\n",
    "        prev_valid_loss = valid_loss\n",
    "\n",
    "        if should_finish:\n",
    "            end = time.time_ns()\n",
    "            print(f'stop training')\n",
    "            best_accuracy = max_valid_acc * 100.0\n",
    "            print(f'achieved best valid accuracy: {best_accuracy}%')\n",
    "            print(f'executed epochs: {epoch}')\n",
    "            elapsed_time = (end - start) / 1000_000_000.0\n",
    "            print(f'ett (elapsed training time)')\n",
    "            print(f'total ett: {elapsed_time} seconds')\n",
    "            print(f'avg ett: {elapsed_time / float(epochs + 1)} seconds')\n",
    "\n",
    "            with open(f'{output_file_path}', 'w') as f:\n",
    "                f.write(f'{epoch + 1} {elapsed_time} {elapsed_time / float(epoch + 1)} {target_valid_accuracy} {best_accuracy}')\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          loss_function,\n",
    "          train_data_loader,\n",
    "          valid_data_loader,\n",
    "          target_valid_accuracy,\n",
    "          epochs,\n",
    "          learning_rate,\n",
    "          input_shape,\n",
    "          use_nimble,\n",
    "          output_file_path):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f'device {device}')\n",
    "\n",
    "    cnt_model = model.count\n",
    "    print(f'model count: {cnt_model}')\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    #\n",
    "    if use_nimble:\n",
    "        dummy_input = torch.randn(input_shape).cuda()\n",
    "        model = torch.cuda.Nimble(model)\n",
    "        model.prepare(dummy_input, training=True)\n",
    "    #\n",
    "\n",
    "    train_losses = 0.0\n",
    "    cnt_train_data = len(train_data_loader)\n",
    "    cnt_valid_data = len(valid_data_loader)\n",
    "\n",
    "    chances = 2\n",
    "    remain_chances = chances\n",
    "\n",
    "    prev_valid_loss = float(\"inf\")\n",
    "\n",
    "    start = time.time_ns()\n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch: {epoch}')\n",
    "        for batch_idx, (input_data, target) in enumerate(train_data_loader, 0):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            outputs = model(input_data)\n",
    "            loss = loss_function(outputs, target)\n",
    "            train_losses += (loss.item() / cnt_model)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10000 == 0:\n",
    "                print(f'avg loss among models: {loss / cnt_model}')\n",
    "\n",
    "        train_loss = train_losses / cnt_train_data\n",
    "\n",
    "        corrects = [0.0] * cnt_model\n",
    "        valid_losses = 0.0\n",
    "        for batch_idx, (input_data, target) in enumerate(valid_data_loader, 0):\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(input_data)\n",
    "\n",
    "            loss = loss_function(output, target)\n",
    "            valid_losses += (loss.item() / cnt_model)\n",
    "\n",
    "            for i in range(cnt_model):\n",
    "                _, predicted = torch.max(output[i], 1) if cnt_model > 1 else torch.max(output, 1)\n",
    "\n",
    "                if predicted == target:\n",
    "                    corrects[i] += 1.0\n",
    "\n",
    "        valid_loss = valid_losses / cnt_valid_data\n",
    "\n",
    "        should_finish = False\n",
    "\n",
    "        valid_accuracies = (np.array(corrects) / cnt_valid_data)\n",
    "\n",
    "        max_valid_index = np.argmax(valid_accuracies)\n",
    "        max_valid_acc = np.max(valid_accuracies)\n",
    "\n",
    "        print(f'max valid accuracy from model #{max_valid_index}: {max_valid_acc * 100.0}')\n",
    "\n",
    "        if max_valid_acc > target_valid_accuracy:\n",
    "            should_finish = True\n",
    "\n",
    "        print(f'train loss: {train_loss}, valid loss: {valid_loss}')\n",
    "        if valid_loss > prev_valid_loss:\n",
    "            if remain_chances == 0:\n",
    "                should_finish = True\n",
    "            else:\n",
    "                remain_chances -= 1\n",
    "        else:\n",
    "            remain_chances = chances\n",
    "\n",
    "        prev_valid_loss = valid_loss\n",
    "\n",
    "        if should_finish:\n",
    "            end = time.time_ns()\n",
    "            print(f'stop training')\n",
    "            best_accuracy = max_valid_acc * 100.0\n",
    "            print(f'achieved best valid accuracy: {best_accuracy}%')\n",
    "            print(f'executed epochs: {epoch}')\n",
    "            elapsed_time = (end - start) / 1000_000_000.0\n",
    "            print(f'ett (elapsed training time)')\n",
    "            print(f'total ett: {elapsed_time} seconds')\n",
    "            print(f'avg ett: {elapsed_time / float(epochs + 1)} seconds')\n",
    "\n",
    "            with open(f'{output_file_path}', 'w') as f:\n",
    "                f.write(f'{epoch + 1} {elapsed_time} {elapsed_time / float(epoch + 1)} {target_valid_accuracy} {best_accuracy}')\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15.3%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "77.9%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "95.3%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/cifar-10/cifar-10-python.tar.gz to ../data/cifar-10\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_root = '../data/cifar-10'\n",
    "batch_size = 1\n",
    "input_width = 32\n",
    "input_height = input_width\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_width, input_height)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                         std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(root=data_root,\n",
    "                              train=True,\n",
    "                              transform=data_transforms,\n",
    "                              download=True)\n",
    "train_data_loader = DataLoader(train_data,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=False)\n",
    "\n",
    "\n",
    "test_data = datasets.CIFAR10(root=data_root,\n",
    "                             train=False,\n",
    "                             transform=data_transforms,\n",
    "                             download=True)\n",
    "test_data_loader = DataLoader(test_data,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "model count: 2\n",
      "epoch: 0\n",
      "avg loss among models: 2.0767369270324707\n",
      "avg loss among models: 1.6453828811645508\n",
      "avg loss among models: 2.262073040008545\n",
      "avg loss among models: 2.341078758239746\n",
      "avg loss among models: 1.745619535446167\n",
      "max valid accuracy from model #1: 59.809999999999995\n",
      "train loss: 1.8377732287168502, valid loss: 1.7612307681918145\n",
      "epoch: 1\n",
      "avg loss among models: 1.9813506603240967\n",
      "avg loss among models: 1.4968984127044678\n",
      "avg loss among models: 2.294503688812256\n",
      "avg loss among models: 2.352879524230957\n",
      "avg loss among models: 1.361149787902832\n",
      "max valid accuracy from model #0: 66.44\n",
      "train loss: 3.5534216199564934, valid loss: 1.7067400129675865\n",
      "epoch: 2\n",
      "avg loss among models: 1.9655776023864746\n",
      "avg loss among models: 1.4884743690490723\n",
      "avg loss among models: 2.287611961364746\n",
      "avg loss among models: 2.210484027862549\n",
      "avg loss among models: 1.3611485958099365\n",
      "max valid accuracy from model #0: 68.51\n",
      "train loss: 5.199470404629707, valid loss: 1.6874809146523475\n",
      "epoch: 3\n",
      "avg loss among models: 1.3614709377288818\n",
      "avg loss among models: 1.373166799545288\n",
      "avg loss among models: 2.187316656112671\n",
      "avg loss among models: 2.3416976928710938\n",
      "avg loss among models: 1.3611705303192139\n",
      "max valid accuracy from model #1: 69.95\n",
      "train loss: 6.79483868748188, valid loss: 1.676231690955162\n",
      "epoch: 4\n",
      "avg loss among models: 1.3611674308776855\n",
      "avg loss among models: 1.3612037897109985\n",
      "avg loss among models: 1.9036915302276611\n",
      "avg loss among models: 2.3385887145996094\n",
      "avg loss among models: 1.3611502647399902\n",
      "max valid accuracy from model #1: 70.00999999999999\n",
      "train loss: 8.351643635911941, valid loss: 1.671392716383934\n",
      "stop training\n",
      "achieved best valid accuracy: 70.00999999999999%\n",
      "executed epochs: 4\n",
      "elapsed training time 544.465660658 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yskim/anaconda3/envs/nimble/lib/python3.7/site-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    }
   ],
   "source": [
    "train(model=MultiModel_2(),\n",
    "      loss_function=OnlineDistillationLoss(),\n",
    "      train_data_loader=train_data_loader,\n",
    "      valid_data_loader=test_data_loader,\n",
    "      target_valid_accuracy=target_valid_accuracy,\n",
    "      epochs=epochs,\n",
    "      learning_rate=lr,\n",
    "      input_shape=[batch_size, 3, input_width, input_height],\n",
    "      use_nimble=True,\n",
    "      output_file_path='multi_model_2.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "model count: 4\n",
      "epoch: 0\n",
      "avg loss among models: 2.0780484676361084\n",
      "avg loss among models: 2.124520778656006\n",
      "avg loss among models: 2.2745704650878906\n",
      "avg loss among models: 2.297234296798706\n",
      "avg loss among models: 1.9206916093826294\n",
      "max valid accuracy from model #3: 60.809999999999995\n",
      "train loss: 1.8365279232549667, valid loss: 1.756417323410511\n",
      "epoch: 1\n",
      "avg loss among models: 1.5281035900115967\n",
      "avg loss among models: 1.3734983205795288\n",
      "avg loss among models: 2.3082122802734375\n",
      "avg loss among models: 2.347811222076416\n",
      "avg loss among models: 1.662687063217163\n",
      "max valid accuracy from model #0: 66.84\n",
      "train loss: 3.5486904705667497, valid loss: 1.7100672584414482\n",
      "epoch: 2\n",
      "avg loss among models: 1.7621984481811523\n",
      "avg loss among models: 1.73323655128479\n",
      "avg loss among models: 2.228783130645752\n",
      "avg loss among models: 2.4068679809570312\n",
      "avg loss among models: 1.6890041828155518\n",
      "max valid accuracy from model #2: 68.73\n",
      "train loss: 5.194493986582756, valid loss: 1.6863631024122239\n",
      "epoch: 3\n",
      "avg loss among models: 1.4505138397216797\n",
      "avg loss among models: 1.4044413566589355\n",
      "avg loss among models: 2.112006187438965\n",
      "avg loss among models: 2.3844058513641357\n",
      "avg loss among models: 1.3611493110656738\n",
      "max valid accuracy from model #0: 69.89\n",
      "train loss: 6.791891281383037, valid loss: 1.6738810761094094\n",
      "epoch: 4\n",
      "avg loss among models: 1.3646290302276611\n",
      "avg loss among models: 1.3686292171478271\n",
      "avg loss among models: 2.1054575443267822\n",
      "avg loss among models: 2.3870849609375\n",
      "avg loss among models: 1.3611536026000977\n",
      "max valid accuracy from model #2: 70.67\n",
      "train loss: 8.35286658201933, valid loss: 1.6787898511767387\n",
      "stop training\n",
      "achieved best valid accuracy: 70.67%\n",
      "executed epochs: 4\n",
      "elapsed training time 1010.677294305 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yskim/anaconda3/envs/nimble/lib/python3.7/site-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    }
   ],
   "source": [
    "train(model=MultiModel_4(),\n",
    "      loss_function=OnlineDistillationLoss(),\n",
    "      train_data_loader=train_data_loader,\n",
    "      valid_data_loader=test_data_loader,\n",
    "      target_valid_accuracy=target_valid_accuracy,\n",
    "      epochs=epochs,\n",
    "      learning_rate=lr,\n",
    "      input_shape=[batch_size, 3, input_width, input_height],\n",
    "      use_nimble=True,\n",
    "      output_file_path='multi_model_4.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(model=MultiModel_8(),\n",
    "      loss_function=OnlineDistillationLoss(),\n",
    "      train_data_loader=train_data_loader,\n",
    "      valid_data_loader=test_data_loader,\n",
    "      target_valid_accuracy=target_valid_accuracy,\n",
    "      epochs=epochs,\n",
    "      learning_rate=lr,\n",
    "      input_shape=[batch_size, 3, input_width, input_height],\n",
    "      use_nimble=True,\n",
    "      output_file_path='multi_model_8.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "epoch: 0\n",
      "avg loss among models: 2.293501138687134\n",
      "avg loss among models: 2.4611458778381348\n",
      "avg loss among models: 2.4429893493652344\n",
      "avg loss among models: 2.4611501693725586\n",
      "avg loss among models: 2.461118698120117\n",
      "max valid accuracy from model #0: 43.97\n",
      "train loss: 2.062392676577568, valid loss: 2.0177929972052575\n",
      "stop training\n",
      "achieved best valid accuracy: 43.97%\n",
      "executed epochs: 0\n",
      "elapsed training time 110.249846087 seconds\n"
     ]
    }
   ],
   "source": [
    "target_valid_accuracy = 0.5\n",
    "\n",
    "train(model=SingleModel(),\n",
    "      loss_function=nn.CrossEntropyLoss(),\n",
    "      train_data_loader=train_data_loader,\n",
    "      valid_data_loader=test_data_loader,\n",
    "      target_valid_accuracy=target_valid_accuracy,\n",
    "      epochs=epochs,\n",
    "      learning_rate=lr,\n",
    "      input_shape=[batch_size, 3, input_width, input_height],\n",
    "      use_nimble=False,\n",
    "      output_file_path='single_model.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import torch.onnx as onnx\n",
    "\n",
    "model = MultiModel()\n",
    "model.eval()\n",
    "\n",
    "x = torch.rand(1, 3, 32,  32, requires_grad=True)\n",
    "out = model(x)\n",
    "\n",
    "onnx.export(model,\n",
    "            x,\n",
    "            \"multi_custom_model.onnx\",\n",
    "            export_params=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output_0', 'output_1'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def train_multi_model():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    use_cuda = True if torch.cuda.is_available() else False\n",
    "    print(f'device {device}')\n",
    "\n",
    "    model = MultiModel().to(device)\n",
    "\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    kld_loss = nn.KLDivLoss(reduction='mean')\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch: {epoch}')\n",
    "        for batch_idx, (input_data, target) in enumerate(train_data_loader, 0):\n",
    "            if batch_idx >= 1:\n",
    "                break\n",
    "\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            with profiler.profile(record_shapes=True,\n",
    "                                  profile_memory=True,\n",
    "                                  use_cuda=use_cuda) as prof:\n",
    "                with profiler.record_function(\"train_multi_custom_model\"):\n",
    "                    output_0, output_1 = model(input_data)\n",
    "\n",
    "                    loss_0 = ce_loss(output_0, target) + kld_loss(output_1.detach().clone(), output_0)\n",
    "                    loss_1 = ce_loss(output_1, target) + kld_loss(output_0.detach().clone(), output_1)\n",
    "\n",
    "                    loss = loss_0 + loss_1\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            if batch_idx % 10000 == 0:\n",
    "                print(f'{loss_0.item()}, {loss_1.item()}')\n",
    "\n",
    "            print(prof.key_averages())\n",
    "            prof.export_chrome_trace(\"trace_multi_custom_model.json\")\n",
    "\n",
    "    correct_0 = 0\n",
    "    correct_1 = 0\n",
    "    total = len(test_data_loader)\n",
    "    for batch_idx, (input_data, target) in enumerate(test_data_loader, 0):\n",
    "        input_data = input_data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output_0, output_1 = model(input_data)\n",
    "        _, predicted = torch.max(output_0, 1)\n",
    "        if predicted == target:\n",
    "            correct_0 += 1\n",
    "\n",
    "        _, predicted = torch.max(output_1, 1)\n",
    "        if predicted == target:\n",
    "            correct_1 += 1\n",
    "\n",
    "    print(correct_0 / total * 100.0)\n",
    "    print(correct_1 / total * 100.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "epoch: 0\n",
      "2.0938072204589844, 2.027334690093994\n",
      "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                   [memory]         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     -16.08 Mb     -16.08 Mb            24  \n",
      "                                aten::zeros         0.17%      24.560us         0.30%      43.288us      14.429us      37.314us         0.26%      43.233us      14.411us          12 b           0 b           0 b           0 b             3  \n",
      "                                aten::empty         2.51%     362.313us         2.51%     362.313us       2.970us       0.000us         0.00%       0.000us       0.000us          60 b          60 b      17.45 Mb      17.45 Mb           122  \n",
      "                                aten::zero_         0.24%      34.440us         0.52%      74.735us       8.304us      31.744us         0.22%      67.872us       7.541us           0 b           0 b           0 b           0 b             9  \n",
      "                   train_multi_custom_model        44.17%       6.381ms        72.89%      10.531ms      10.531ms       6.272ms        42.99%      10.578ms      10.578ms          -4 b         -20 b       1.14 Mb      -4.50 Kb             1  \n",
      "                               aten::conv2d         0.19%      27.543us         3.59%     518.708us     129.677us      21.952us         0.15%     488.480us     122.120us           0 b           0 b     384.00 Kb           0 b             4  \n",
      "                          aten::convolution         0.18%      25.781us         3.40%     491.165us     122.791us      19.488us         0.13%     466.528us     116.632us           0 b           0 b     384.00 Kb           0 b             4  \n",
      "                         aten::_convolution         0.43%      61.933us         3.22%     465.384us     116.346us      42.048us         0.29%     447.040us     111.760us           0 b           0 b     384.00 Kb           0 b             4  \n",
      "                    aten::cudnn_convolution         1.94%     280.978us         2.03%     292.686us      73.171us     340.416us         2.33%     340.416us      85.104us           0 b           0 b     384.00 Kb     384.00 Kb             4  \n",
      "                              aten::resize_         0.55%      78.953us         0.55%      78.953us       1.974us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     295.00 Kb     295.00 Kb            40  \n",
      "                              aten::reshape         0.25%      35.803us         0.54%      77.844us       7.784us      46.176us         0.32%      46.176us       4.618us           0 b           0 b           0 b           0 b            10  \n",
      "                                 aten::view         0.44%      63.679us         0.44%      63.679us       3.184us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            20  \n",
      "                                 aten::add_         4.59%     663.392us         4.59%     663.392us      22.113us     713.922us         4.89%     713.922us      23.797us           0 b           0 b           0 b           0 b            30  \n",
      "                                 aten::relu         0.58%      83.996us         1.24%     178.672us      29.779us      62.240us         0.43%     141.024us      23.504us           0 b           0 b     388.00 Kb           0 b             6  \n",
      "                            aten::threshold         0.45%      65.511us         0.66%      94.676us      15.779us      78.784us         0.54%      78.784us      13.131us           0 b           0 b     388.00 Kb           0 b             6  \n",
      "                           aten::max_pool2d         0.18%      25.452us         1.12%     161.758us      40.439us      23.840us         0.16%     159.712us      39.928us           0 b           0 b     288.00 Kb           0 b             4  \n",
      "              aten::max_pool2d_with_indices         0.68%      98.237us         0.94%     136.306us      34.076us     135.872us         0.93%     135.872us      33.968us           0 b           0 b     288.00 Kb           0 b             4  \n",
      "                                  aten::add         0.90%     130.266us         0.90%     130.266us      18.609us     116.512us         0.80%     116.512us      16.645us           0 b           0 b       3.50 Kb       3.50 Kb             7  \n",
      "                           aten::batch_norm         0.17%      24.312us         1.84%     265.439us      66.360us      21.344us         0.15%     262.976us      65.744us           0 b           0 b     100.00 Kb           0 b             4  \n",
      "               aten::_batch_norm_impl_index         0.18%      25.927us         1.67%     241.127us      60.282us      26.240us         0.18%     241.632us      60.408us           0 b           0 b     100.00 Kb           0 b             4  \n",
      "                     aten::cudnn_batch_norm         1.03%     148.843us         1.49%     215.200us      53.800us     215.392us         1.48%     215.392us      53.848us           0 b           0 b     100.00 Kb           0 b             4  \n",
      "                           aten::empty_like         0.31%      44.660us         0.85%     122.699us       6.458us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     485.50 Kb           0 b            19  \n",
      "                              aten::flatten         0.07%       9.689us         0.14%      19.735us       9.868us      18.976us         0.13%      18.976us       9.488us           0 b           0 b           0 b           0 b             2  \n",
      "                               aten::linear         0.21%      29.747us         1.95%     282.115us      70.529us      60.992us         0.42%     472.960us     118.240us           0 b           0 b       5.00 Kb           0 b             4  \n",
      "                                    aten::t         0.45%      65.277us         0.83%     119.454us       5.973us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            20  \n",
      "                            aten::transpose         0.25%      35.572us         0.38%      54.177us       2.709us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            20  \n",
      "                           aten::as_strided         0.16%      23.591us         0.16%      23.591us       0.907us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            26  \n",
      "                                aten::addmm         1.26%     182.163us         1.50%     217.011us      54.253us     411.968us         2.82%     411.968us     102.992us           0 b           0 b       5.00 Kb           0 b             4  \n",
      "                               aten::expand         0.06%       8.271us         0.08%      11.366us       2.842us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             4  \n",
      "                              aten::softmax         0.10%      13.784us         0.51%      74.232us      37.116us       6.144us         0.04%      37.088us      18.544us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                             aten::_softmax         0.30%      43.040us         0.42%      60.448us      30.224us      30.944us         0.21%      30.944us      15.472us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                          aten::log_softmax         0.08%      11.701us         0.40%      57.980us      28.990us       8.480us         0.06%      38.240us      19.120us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                         aten::_log_softmax         0.23%      33.685us         0.32%      46.279us      23.139us      29.760us         0.20%      29.760us      14.880us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                             aten::nll_loss         0.08%      11.093us         0.43%      61.488us      30.744us      10.432us         0.07%      60.865us      30.432us           0 b           0 b       2.00 Kb           0 b             2  \n",
      "                     aten::nll_loss_forward         0.35%      50.395us         0.35%      50.395us      25.197us      50.432us         0.35%      50.432us      25.216us           0 b           0 b       2.00 Kb       2.00 Kb             2  \n",
      "                               aten::detach         0.07%       9.744us         0.10%      15.149us       7.575us       9.152us         0.06%      14.272us       7.136us           0 b           0 b           0 b           0 b             2  \n",
      "                                     detach         0.04%       5.405us         0.04%       5.405us       2.702us       5.120us         0.04%       5.120us       2.560us           0 b           0 b           0 b           0 b             2  \n",
      "                                aten::clone         0.17%      25.066us         0.40%      58.331us      29.166us      34.369us         0.24%      58.688us      29.344us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                        aten::empty_strided         0.83%     119.995us         0.83%     119.995us       4.444us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      16.09 Mb      16.09 Mb            27  \n",
      "                                aten::copy_         2.42%     349.901us         2.42%     349.901us      13.458us     419.359us         2.87%     419.359us      16.129us           0 b           0 b           0 b           0 b            26  \n",
      "                               aten::kl_div         3.59%     518.981us        12.67%       1.831ms     915.364us       1.023ms         7.01%       1.892ms     945.776us           0 b           0 b       1.00 Kb      -6.00 Kb             2  \n",
      "                                  aten::log         2.99%     432.059us         3.13%     452.348us     113.087us     516.608us         3.54%     516.608us     129.152us           0 b           0 b       2.00 Kb           0 b             4  \n",
      "                                  aten::sub         0.13%      18.401us         0.18%      26.694us      13.347us      17.472us         0.12%      17.472us       8.736us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                                  aten::mul         0.44%      63.624us         0.59%      85.874us      14.312us      78.112us         0.54%      78.112us      13.019us           0 b           0 b       3.00 Kb           0 b             6  \n",
      "                           aten::zeros_like         0.22%      31.350us         0.92%     133.331us      22.222us      56.866us         0.39%     118.819us      19.803us           0 b           0 b     385.00 Kb           0 b             6  \n",
      "                                aten::fill_         0.33%      47.685us         0.33%      47.685us       6.812us      43.872us         0.30%      43.872us       6.267us           0 b           0 b           0 b           0 b             7  \n",
      "                                   aten::gt         0.35%      50.029us         0.82%     118.337us      29.584us      42.944us         0.29%      84.544us      21.136us           0 b           0 b       2.00 Kb           0 b             4  \n",
      "                                 aten::item         0.11%      16.576us         0.17%      24.319us       6.080us      14.079us         0.10%      21.919us       5.480us           0 b           0 b           0 b           0 b             4  \n",
      "                  aten::_local_scalar_dense         0.05%       7.743us         0.05%       7.743us       1.936us       7.840us         0.05%       7.840us       1.960us           0 b           0 b           0 b           0 b             4  \n",
      "                                aten::where         0.08%      11.956us         2.69%     388.664us     194.332us      10.720us         0.07%      26.944us      13.472us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                             aten::_s_where         2.55%     369.102us         2.61%     376.708us     188.354us      16.224us         0.11%      16.224us       8.112us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                                 aten::mean         3.68%     531.001us         3.74%     539.708us     269.854us     446.112us         3.06%     446.112us     223.056us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "          Optimizer.zero_grad#SGD.zero_grad         0.18%      26.135us         0.20%      29.108us      29.108us      29.152us         0.20%      29.152us      29.152us          -4 b         -20 b           0 b           0 b             1  \n",
      "                            aten::ones_like         0.04%       6.403us         0.15%      21.274us      21.274us      13.695us         0.09%      21.439us      21.439us           0 b           0 b         512 b           0 b             1  \n",
      "                               AddBackward0         0.24%      34.037us         0.24%      34.037us       4.862us      31.967us         0.22%      31.967us       4.567us           0 b           0 b           0 b           0 b             7  \n",
      "                              KlDivBackward         0.46%      67.139us        11.70%       1.690ms     844.858us     127.393us         0.87%       1.690ms     844.944us           0 b           0 b       1.00 Kb      -2.00 Kb             2  \n",
      "                                 aten::sub_         0.16%      22.939us         0.16%      22.939us      11.469us      21.472us         0.15%      21.472us      10.736us           0 b           0 b           0 b           0 b             2  \n",
      "                                   aten::eq         0.39%      56.581us         0.88%     127.242us      31.811us      65.824us         0.45%     127.041us      31.760us           0 b           0 b       2.00 Kb           0 b             4  \n",
      "                         aten::masked_fill_         4.77%     688.570us         4.77%     688.570us     344.285us     687.776us         4.71%     687.776us     343.888us           0 b           0 b           0 b           0 b             2  \n",
      "                                 aten::div_         1.38%     199.797us         1.38%     199.797us      99.898us     174.560us         1.20%     174.560us      87.280us           0 b           0 b           0 b           0 b             2  \n",
      "                            NllLossBackward         0.11%      15.251us         0.39%      56.042us      28.021us      14.655us         0.10%      54.879us      27.440us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                    aten::nll_loss_backward         0.28%      40.791us         0.28%      40.791us      20.396us      40.224us         0.28%      40.224us      20.112us           0 b           0 b       1.00 Kb       1.00 Kb             2  \n",
      "                         LogSoftmaxBackward         0.09%      12.752us         0.35%      50.123us      25.061us      12.449us         0.09%      49.312us      24.656us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "           aten::_log_softmax_backward_data         0.17%      25.000us         0.26%      37.371us      18.686us      36.864us         0.25%      36.864us      18.432us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                            SoftmaxBackward         0.10%      14.703us         0.58%      83.670us      41.835us      12.287us         0.08%      78.560us      39.280us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "               aten::_softmax_backward_data         0.24%      33.958us         0.48%      68.967us      34.484us      41.472us         0.28%      66.272us      33.136us           0 b           0 b       1.00 Kb      -1.00 Kb             2  \n",
      "                              AddmmBackward         0.48%      69.361us         2.43%     350.756us      87.689us     108.163us         0.74%     438.403us     109.601us           0 b           0 b      15.70 Mb           0 b             4  \n",
      "                                 aten::conj         0.14%      20.525us         0.14%      20.525us       2.566us      16.352us         0.11%      16.352us       2.044us           0 b           0 b           0 b           0 b             8  \n",
      "                                   aten::mm         1.16%     167.069us         1.37%     197.430us      24.679us     313.888us         2.15%     313.888us      39.236us           0 b           0 b      15.70 Mb           0 b             8  \n",
      "            torch::autograd::AccumulateGrad         1.28%     185.572us         4.86%     701.577us      29.232us     155.651us         1.07%     701.475us      29.228us           0 b           0 b      16.08 Mb           0 b            24  \n",
      "                    aten::new_empty_strided         0.58%      84.302us         1.31%     188.790us       7.866us     150.785us         1.03%     150.785us       6.283us           0 b           0 b      16.08 Mb           0 b            24  \n",
      "                                  TBackward         0.08%      12.071us         0.23%      32.728us       8.182us      21.153us         0.15%      21.153us       5.288us           0 b           0 b           0 b           0 b             4  \n",
      "                              ReluBackward0         0.24%      34.705us         0.98%     141.943us      23.657us      31.840us         0.22%     139.423us      23.237us           0 b           0 b     388.00 Kb           0 b             6  \n",
      "                   aten::threshold_backward         0.56%      80.976us         0.74%     107.238us      17.873us     107.583us         0.74%     107.583us      17.930us           0 b           0 b     388.00 Kb           0 b             6  \n",
      "                               ViewBackward         0.22%      31.288us         0.50%      72.320us      12.053us      23.677us         0.16%      55.581us       9.264us           0 b           0 b           0 b           0 b             6  \n",
      "                     CudnnBatchNormBackward         0.24%      35.319us         1.27%     182.970us      45.743us      20.033us         0.14%     119.520us      29.880us           0 b           0 b     100.00 Kb           0 b             4  \n",
      "            aten::cudnn_batch_norm_backward         0.70%     101.098us         1.02%     147.651us      36.913us      99.487us         0.68%      99.487us      24.872us           0 b           0 b     100.00 Kb           0 b             4  \n",
      "               MaxPool2DWithIndicesBackward         0.20%      28.544us         1.38%     199.350us      49.838us      23.682us         0.16%     199.809us      49.952us           0 b           0 b     384.00 Kb           0 b             4  \n",
      "     aten::max_pool2d_with_indices_backward         0.47%      67.916us         1.18%     170.806us      42.701us      72.124us         0.49%     176.127us      44.032us           0 b           0 b     384.00 Kb           0 b             4  \n",
      "                           aten::resize_as_         0.09%      12.677us         0.10%      14.667us       3.667us      15.296us         0.10%      15.296us       3.824us           0 b           0 b           0 b           0 b             4  \n",
      "                   CudnnConvolutionBackward         0.22%      31.100us         2.10%     303.283us      75.821us      26.688us         0.18%     412.575us     103.144us           0 b           0 b     483.00 Kb           0 b             4  \n",
      "           aten::cudnn_convolution_backward         0.37%      52.963us         1.88%     272.183us      68.046us      37.278us         0.26%     385.888us      96.472us           0 b           0 b     483.00 Kb           0 b             4  \n",
      "     aten::cudnn_convolution_backward_input         0.53%      76.951us         0.56%      81.268us      40.634us     145.889us         1.00%     145.889us      72.944us           0 b           0 b      64.00 Kb      64.00 Kb             2  \n",
      "    aten::cudnn_convolution_backward_weight         0.80%     115.746us         0.95%     137.952us      34.488us     202.721us         1.39%     202.721us      50.680us           0 b           0 b     419.00 Kb           0 b             4  \n",
      "                    Optimizer.step#SGD.step         1.25%     180.081us         2.87%     414.800us     414.800us     131.103us         0.90%     468.353us     468.353us          -4 b         -20 b           0 b           0 b             1  \n",
      "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 14.447ms\n",
      "Self CUDA time total: 14.588ms\n",
      "\n",
      "9.62\n",
      "11.379999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yskim/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py:2610: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#train_multi_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_resnet18():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f'device {device}')\n",
    "\n",
    "    model = models.resnet18()\n",
    "    model.fc = nn.Linear(512, 10)\n",
    "    model = model.to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch: {epoch}')\n",
    "        for batch_idx, (input_data, target) in enumerate(train_data_loader, 0):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(input_data)\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10000 == 0:\n",
    "                print(loss)\n",
    "\n",
    "    correct = 0\n",
    "    total = len(test_data_loader)\n",
    "    for batch_idx, (input_data, target) in enumerate(test_data_loader, 0):\n",
    "        input_data = input_data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(input_data)\n",
    "\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "        if predicted == target:\n",
    "            correct += 1\n",
    "    print(correct / total * 100.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#train_resnet18()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "\n",
    "    plt.imshow(np.transpose(npimg.reshape(3,32,32), (1,2,0)).copy())\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}